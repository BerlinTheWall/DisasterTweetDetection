{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# contractions_dict = {\n",
    "#     \"ain't\": \"are not\",\n",
    "#     \"aren't\": \"are not\",\n",
    "#     \"can't\": \"cannot\",\n",
    "#     \"couldn't\": \"could not\",\n",
    "#     \"could've\": \"could have\",\n",
    "#     \"didn't\": \"did not\",\n",
    "#     \"doesn't\": \"does not\",\n",
    "#     \"don't\": \"do not\",\n",
    "#     \"hadn't\": \"had not\",\n",
    "#     \"hasn't\": \"has not\",\n",
    "#     \"haven't\": \"have not\",\n",
    "#     \"he's\": \"he is\",\n",
    "#     \"he'll\": \"he will\",\n",
    "#     \"he'd\": \"he would\",\n",
    "#     \"here's\": \"here is\",\n",
    "#     \"how's\": \"how is\",\n",
    "#     \"I'll\": \"I will\",\n",
    "#     \"I'm\": \"I am\",\n",
    "#     \"I've\": \"I have\",\n",
    "#     \"isn't\": \"is not\",\n",
    "#     \"it's\": \"it is\",\n",
    "#     \"let's\": \"let us\",\n",
    "#     \"ma'am\": \"madam\",\n",
    "#     \"mightn't\": \"might not\",\n",
    "#     \"mustn't\": \"must not\",\n",
    "#     \"must've\": \"must have\",\n",
    "#     \"needn't\": \"need not\",\n",
    "#     \"shan't\": \"shall not\",\n",
    "#     \"she's\": \"she is\",\n",
    "#     \"shouldn't\": \"should not\",\n",
    "#     \"should've\": \"should have\",\n",
    "#     \"that's\": \"that is\",\n",
    "#     \"there's\": \"there is\",\n",
    "#     \"they're\": \"they are\",\n",
    "#     \"they've\": \"they have\",\n",
    "#     \"we're\": \"we are\",\n",
    "#     \"we've\": \"we have\",\n",
    "#     \"weren't\": \"were not\",\n",
    "#     \"what's\": \"what is\",\n",
    "#     \"who's\": \"who is\",\n",
    "#     \"won't\": \"will not\",\n",
    "#     \"wouldn't\": \"would not\",\n",
    "#     \"you're\": \"you are\",\n",
    "#     \"you've\": \"you have\",\n",
    "# }\n",
    "\n",
    "# contractions_dict = {\n",
    "#     \"I'd\": \"I would\",\n",
    "#     \"I'd've\": \"I would have\",\n",
    "#     \"I'll\": \"I will\",\n",
    "#     \"I'm\": \"I am\",\n",
    "#     \"I've\": \"I have\",\n",
    "#     \"I'ven't\": \"I have not\",\n",
    "#     \"it'd\": \"it would\",\n",
    "#     \"it'd've\": \"it would have\",\n",
    "#     \"it'll\": \"it will\",\n",
    "#     \"it's\": \"it is\",\n",
    "#     \"let's\": \"let us\",\n",
    "#     \"ma'am\": \"madam\",\n",
    "#     \"mightn't\": \"might not\",\n",
    "#     \"must've\": \"must have\",\n",
    "#     \"needn't\": \"need not\",\n",
    "#     \"o'clock\": \"of the clock\",\n",
    "#     \"she'd\": \"she would\",\n",
    "#     \"she'd've\": \"she would have\",\n",
    "#     \"she'll\": \"she will\",\n",
    "#     \"she's\": \"she is\",\n",
    "#     \"should've\": \"should have\",\n",
    "#     \"that's\": \"that is\",\n",
    "#     \"there's\": \"there is\",\n",
    "#     \"they'd\": \"they would\",\n",
    "#     \"they'd've\": \"they would have\",\n",
    "#     \"they'll\": \"they will\",\n",
    "#     \"they're\": \"they are\",\n",
    "#     \"they've\": \"they have\",\n",
    "#     \"we'd\": \"we would\",\n",
    "#     \"we'd've\": \"we would have\",\n",
    "#     \"we'll\": \"we will\",\n",
    "#     \"we're\": \"we are\",\n",
    "#     \"we've\": \"we have\",\n",
    "#     \"what'll\": \"what will\",\n",
    "#     \"what're\": \"what are\",\n",
    "#     \"what's\": \"what is\",\n",
    "#     \"what've\": \"what have\",\n",
    "#     \"where's\": \"where is\",\n",
    "#     \"who'd\": \"who would\",\n",
    "#     \"who'd've\": \"who would have\",\n",
    "#     \"who'll\": \"who will\",\n",
    "#     \"who're\": \"who are\",\n",
    "#     \"who's\": \"who is\",\n",
    "#     \"who've\": \"who have\",\n",
    "#     \"won't\": \"will not\",\n",
    "#     \"would've\": \"would have\",\n",
    "#     \"y'all\": \"you all\",\n",
    "#     \"you'd\": \"you would\",\n",
    "#     \"you'd've\": \"you would have\",\n",
    "#     \"you'll\": \"you will\",\n",
    "#     \"you're\": \"you are\",\n",
    "#     \"you've\": \"you have\",\n",
    "# }\n",
    "def expand_contractions(input_str):\n",
    "    input_str = re.sub(r\"he's\", \"he is\", input_str)\n",
    "    input_str = re.sub(r\"there's\", \"there is\", input_str)\n",
    "    input_str = re.sub(r\"We're\", \"We are\", input_str)\n",
    "    input_str = re.sub(r\"That's\", \"That is\", input_str)\n",
    "    input_str = re.sub(r\"won't\", \"will not\", input_str)\n",
    "    input_str = re.sub(r\"they're\", \"they are\", input_str)\n",
    "    input_str = re.sub(r\"Can't\", \"Cannot\", input_str)\n",
    "    input_str = re.sub(r\"wasn't\", \"was not\", input_str)\n",
    "    input_str = re.sub(r\"don\\x89Ûªt\", \"do not\", input_str)\n",
    "    input_str = re.sub(r\"aren't\", \"are not\", input_str)\n",
    "    input_str = re.sub(r\"isn't\", \"is not\", input_str)\n",
    "    input_str = re.sub(r\"What's\", \"What is\", input_str)\n",
    "    input_str = re.sub(r\"haven't\", \"have not\", input_str)\n",
    "    input_str = re.sub(r\"hasn't\", \"has not\", input_str)\n",
    "    input_str = re.sub(r\"There's\", \"There is\", input_str)\n",
    "    input_str = re.sub(r\"He's\", \"He is\", input_str)\n",
    "    input_str = re.sub(r\"It's\", \"It is\", input_str)\n",
    "    input_str = re.sub(r\"You're\", \"You are\", input_str)\n",
    "    input_str = re.sub(r\"I'M\", \"I am\", input_str)\n",
    "    input_str = re.sub(r\"shouldn't\", \"should not\", input_str)\n",
    "    input_str = re.sub(r\"wouldn't\", \"would not\", input_str)\n",
    "    input_str = re.sub(r\"i'm\", \"I am\", input_str)\n",
    "    input_str = re.sub(r\"I\\x89Ûªm\", \"I am\", input_str)\n",
    "    input_str = re.sub(r\"I'm\", \"I am\", input_str)\n",
    "    input_str = re.sub(r\"Isn't\", \"is not\", input_str)\n",
    "    input_str = re.sub(r\"Here's\", \"Here is\", input_str)\n",
    "    input_str = re.sub(r\"you've\", \"you have\", input_str)\n",
    "    input_str = re.sub(r\"you\\x89Ûªve\", \"you have\", input_str)\n",
    "    input_str = re.sub(r\"we're\", \"we are\", input_str)\n",
    "    input_str = re.sub(r\"what's\", \"what is\", input_str)\n",
    "    input_str = re.sub(r\"couldn't\", \"could not\", input_str)\n",
    "    input_str = re.sub(r\"we've\", \"we have\", input_str)\n",
    "    input_str = re.sub(r\"it\\x89Ûªs\", \"it is\", input_str)\n",
    "    input_str = re.sub(r\"doesn\\x89Ûªt\", \"does not\", input_str)\n",
    "    input_str = re.sub(r\"It\\x89Ûªs\", \"It is\", input_str)\n",
    "    input_str = re.sub(r\"Here\\x89Ûªs\", \"Here is\", input_str)\n",
    "    input_str = re.sub(r\"who's\", \"who is\", input_str)\n",
    "    input_str = re.sub(r\"I\\x89Ûªve\", \"I have\", input_str)\n",
    "    input_str = re.sub(r\"y'all\", \"you all\", input_str)\n",
    "    input_str = re.sub(r\"can\\x89Ûªt\", \"cannot\", input_str)\n",
    "    input_str = re.sub(r\"would've\", \"would have\", input_str)\n",
    "    input_str = re.sub(r\"it'll\", \"it will\", input_str)\n",
    "    input_str = re.sub(r\"we'll\", \"we will\", input_str)\n",
    "    input_str = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", input_str)\n",
    "    input_str = re.sub(r\"We've\", \"We have\", input_str)\n",
    "    input_str = re.sub(r\"he'll\", \"he will\", input_str)\n",
    "    input_str = re.sub(r\"Y'all\", \"You all\", input_str)\n",
    "    input_str = re.sub(r\"Weren't\", \"Were not\", input_str)\n",
    "    input_str = re.sub(r\"Didn't\", \"Did not\", input_str)\n",
    "    input_str = re.sub(r\"they'll\", \"they will\", input_str)\n",
    "    input_str = re.sub(r\"they'd\", \"they would\", input_str)\n",
    "    input_str = re.sub(r\"DON'T\", \"DO NOT\", input_str)\n",
    "    input_str = re.sub(r\"That\\x89Ûªs\", \"That is\", input_str)\n",
    "    input_str = re.sub(r\"they've\", \"they have\", input_str)\n",
    "    input_str = re.sub(r\"i'd\", \"I would\", input_str)\n",
    "    input_str = re.sub(r\"should've\", \"should have\", input_str)\n",
    "    input_str = re.sub(r\"You\\x89Ûªre\", \"You are\", input_str)\n",
    "    input_str = re.sub(r\"where's\", \"where is\", input_str)\n",
    "    input_str = re.sub(r\"Don\\x89Ûªt\", \"Do not\", input_str)\n",
    "    input_str = re.sub(r\"we'd\", \"we would\", input_str)\n",
    "    input_str = re.sub(r\"i'll\", \"I will\", input_str)\n",
    "    input_str = re.sub(r\"weren't\", \"were not\", input_str)\n",
    "    input_str = re.sub(r\"They're\", \"They are\", input_str)\n",
    "    input_str = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", input_str)\n",
    "    input_str = re.sub(r\"you\\x89Ûªll\", \"you will\", input_str)\n",
    "    input_str = re.sub(r\"I\\x89Ûªd\", \"I would\", input_str)\n",
    "    input_str = re.sub(r\"let's\", \"let us\", input_str)\n",
    "    input_str = re.sub(r\"it's\", \"it is\", input_str)\n",
    "    input_str = re.sub(r\"can't\", \"cannot\", input_str)\n",
    "    input_str = re.sub(r\"don't\", \"do not\", input_str)\n",
    "    input_str = re.sub(r\"you're\", \"you are\", input_str)\n",
    "    input_str = re.sub(r\"i've\", \"I have\", input_str)\n",
    "    input_str = re.sub(r\"that's\", \"that is\", input_str)\n",
    "    input_str = re.sub(r\"i'll\", \"I will\", input_str)\n",
    "    input_str = re.sub(r\"doesn't\", \"does not\", input_str)\n",
    "    input_str = re.sub(r\"i'd\", \"I would\", input_str)\n",
    "    input_str = re.sub(r\"didn't\", \"did not\", input_str)\n",
    "    input_str = re.sub(r\"ain't\", \"am not\", input_str)\n",
    "    input_str = re.sub(r\"you'll\", \"you will\", input_str)\n",
    "    input_str = re.sub(r\"I've\", \"I have\", input_str)\n",
    "    input_str = re.sub(r\"Don't\", \"do not\", input_str)\n",
    "    input_str = re.sub(r\"I'll\", \"I will\", input_str)\n",
    "    input_str = re.sub(r\"I'd\", \"I would\", input_str)\n",
    "    input_str = re.sub(r\"Let's\", \"Let us\", input_str)\n",
    "    input_str = re.sub(r\"you'd\", \"You would\", input_str)\n",
    "    input_str = re.sub(r\"It's\", \"It is\", input_str)\n",
    "    input_str = re.sub(r\"Ain't\", \"am not\", input_str)\n",
    "    input_str = re.sub(r\"Haven't\", \"Have not\", input_str)\n",
    "    input_str = re.sub(r\"Could've\", \"Could have\", input_str)\n",
    "    input_str = re.sub(r\"youve\", \"you have\", input_str)\n",
    "    input_str = re.sub(r\"donå«t\", \"do not\", input_str)\n",
    "\n",
    "    return input_str\n",
    "\n",
    "\n",
    "def remove_urls(input_str):\n",
    "    input_str = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", input_str)\n",
    "    return input_str\n",
    "\n",
    "\n",
    "def def(input_str):\n",
    "    return input_str\n",
    "\n",
    "\n",
    "def separate_punctuations_from_words(input_str):\n",
    "    punctuations = string.punctuation\n",
    "    for p in punctuations:\n",
    "        input_str = input_str.replace(p, f' {p} ')\n",
    "    return input_str\n",
    "\n",
    "\n",
    "def replace_continues_dots(input_str):\n",
    "    continues_dots = ['..', '...', '....', '.....', '......', '.......', '........', '.........', '..........', ]\n",
    "    for c in continues_dots:\n",
    "        input_str = input_str.replace(c, ' ... ')\n",
    "\n",
    "    return input_str\n",
    "\n",
    "\n",
    "def remove_html_tags(input_str):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', input_str)\n",
    "\n",
    "\n",
    "# df['text']=df['text'].apply(lambda x : remove_html(x))\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(input_str):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', input_str)\n",
    "\n",
    "# df['text']=df['text'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "def correct_spellings(input_str):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "\n",
    "text = \"corect me plese\"\n",
    "correct_spellings(text)\n",
    "glove_embeddings = np.load('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', allow_pickle=True)\n",
    "fasttext_embeddings = np.load('../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl',\n",
    "                              allow_pickle=True)\n",
    "\n",
    "\n",
    "def build_vocab(X):\n",
    "    tweets = X.apply(lambda s: s.split()).values\n",
    "    vocab = {}\n",
    "\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_embeddings_coverage(X, embeddings):\n",
    "    vocab = build_vocab(X)\n",
    "\n",
    "    covered = {}\n",
    "    oov = {}\n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "\n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "\n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov, vocab_coverage, text_coverage\n",
    "\n",
    "\n",
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(df_train['text'],\n",
    "                                                                                                   glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(df_test['text'],\n",
    "                                                                                                glove_embeddings)\n",
    "print(\n",
    "    'GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage,\n",
    "                                                                                            train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage,\n",
    "                                                                                          test_glove_text_coverage))\n",
    "\n",
    "train_fasttext_oov, train_fasttext_vocab_coverage, train_fasttext_text_coverage = check_embeddings_coverage(\n",
    "    df_train['text'], fasttext_embeddings)\n",
    "test_fasttext_oov, test_fasttext_vocab_coverage, test_fasttext_text_coverage = check_embeddings_coverage(\n",
    "    df_test['text'], fasttext_embeddings)\n",
    "print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(\n",
    "    train_fasttext_vocab_coverage, train_fasttext_text_coverage))\n",
    "print(\n",
    "    'FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_fasttext_vocab_coverage,\n",
    "                                                                                           test_fasttext_text_coverage))\n",
    "# n_gram?\n",
    "# maybe some common methods for showing stop words or etc?\n",
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "abbreviations = {\n",
    "    \"$\": \" dollar \",\n",
    "    \"€\": \" euro \",\n",
    "    \"4ao\": \"for adults only\",\n",
    "    \"a.m\": \"before midday\",\n",
    "    \"a3\": \"anytime anywhere anyplace\",\n",
    "    \"aamof\": \"as a matter of fact\",\n",
    "    \"acct\": \"account\",\n",
    "    \"adih\": \"another day in hell\",\n",
    "    \"afaic\": \"as far as i am concerned\",\n",
    "    \"afaict\": \"as far as i can tell\",\n",
    "    \"afaik\": \"as far as i know\",\n",
    "    \"afair\": \"as far as i remember\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"app\": \"application\",\n",
    "    \"approx\": \"approximately\",\n",
    "    \"apps\": \"applications\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"asl\": \"age, sex, location\",\n",
    "    \"atk\": \"at the keyboard\",\n",
    "    \"ave.\": \"avenue\",\n",
    "    \"aymm\": \"are you my mother\",\n",
    "    \"ayor\": \"at your own risk\",\n",
    "    \"b&b\": \"bed and breakfast\",\n",
    "    \"b+b\": \"bed and breakfast\",\n",
    "    \"b.c\": \"before christ\",\n",
    "    \"b2b\": \"business to business\",\n",
    "    \"b2c\": \"business to customer\",\n",
    "    \"b4\": \"before\",\n",
    "    \"b4n\": \"bye for now\",\n",
    "    \"b@u\": \"back at you\",\n",
    "    \"bae\": \"before anyone else\",\n",
    "    \"bak\": \"back at keyboard\",\n",
    "    \"bbbg\": \"bye bye be good\",\n",
    "    \"bbc\": \"british broadcasting corporation\",\n",
    "    \"bbias\": \"be back in a second\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"bbs\": \"be back soon\",\n",
    "    \"be4\": \"before\",\n",
    "    \"bfn\": \"bye for now\",\n",
    "    \"blvd\": \"boulevard\",\n",
    "    \"bout\": \"about\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"bros\": \"brothers\",\n",
    "    \"brt\": \"be right there\",\n",
    "    \"bsaaw\": \"big smile and a wink\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"bwl\": \"bursting with laughter\",\n",
    "    \"c/o\": \"care of\",\n",
    "    \"cet\": \"central european time\",\n",
    "    \"cf\": \"compare\",\n",
    "    \"cia\": \"central intelligence agency\",\n",
    "    \"csl\": \"can not stop laughing\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cul8r\": \"see you later\",\n",
    "    \"cv\": \"curriculum vitae\",\n",
    "    \"cwot\": \"complete waste of time\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"cyt\": \"see you tomorrow\",\n",
    "    \"dae\": \"does anyone else\",\n",
    "    \"dbmib\": \"do not bother me i am busy\",\n",
    "    \"diy\": \"do it yourself\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"dwh\": \"during work hours\",\n",
    "    \"e123\": \"easy as one two three\",\n",
    "    \"eet\": \"eastern european time\",\n",
    "    \"eg\": \"example\",\n",
    "    \"embm\": \"early morning business meeting\",\n",
    "    \"encl\": \"enclosed\",\n",
    "    \"encl.\": \"enclosed\",\n",
    "    \"etc\": \"and so on\",\n",
    "    \"faq\": \"frequently asked questions\",\n",
    "    \"fawc\": \"for anyone who cares\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"fc\": \"fingers crossed\",\n",
    "    \"fig\": \"figure\",\n",
    "    \"fimh\": \"forever in my heart\",\n",
    "    \"ft.\": \"feet\",\n",
    "    \"ft\": \"featuring\",\n",
    "    \"ftl\": \"for the loss\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"fwiw\": \"for what it is worth\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"g9\": \"genius\",\n",
    "    \"gahoy\": \"get a hold of yourself\",\n",
    "    \"gal\": \"get a life\",\n",
    "    \"gcse\": \"general certificate of secondary education\",\n",
    "    \"gfn\": \"gone for now\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"gl\": \"good luck\",\n",
    "    \"glhf\": \"good luck have fun\",\n",
    "    \"gmt\": \"greenwich mean time\",\n",
    "    \"gmta\": \"great minds think alike\",\n",
    "    \"gn\": \"good night\",\n",
    "    \"g.o.a.t\": \"greatest of all time\",\n",
    "    \"goat\": \"greatest of all time\",\n",
    "    \"goi\": \"get over it\",\n",
    "    \"gps\": \"global positioning system\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"gratz\": \"congratulations\",\n",
    "    \"gyal\": \"girl\",\n",
    "    \"h&c\": \"hot and cold\",\n",
    "    \"hp\": \"horsepower\",\n",
    "    \"hr\": \"hour\",\n",
    "    \"hrh\": \"his royal highness\",\n",
    "    \"ht\": \"height\",\n",
    "    \"ibrb\": \"i will be right back\",\n",
    "    \"ic\": \"i see\",\n",
    "    \"icq\": \"i seek you\",\n",
    "    \"icymi\": \"in case you missed it\",\n",
    "    \"idc\": \"i do not care\",\n",
    "    \"idgadf\": \"i do not give a damn fuck\",\n",
    "    \"idgaf\": \"i do not give a fuck\",\n",
    "    \"idk\": \"i do not know\",\n",
    "    \"ie\": \"that is\",\n",
    "    \"i.e\": \"that is\",\n",
    "    \"ifyp\": \"i feel your pain\",\n",
    "    \"IG\": \"instagram\",\n",
    "    \"iirc\": \"if i remember correctly\",\n",
    "    \"ilu\": \"i love you\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imu\": \"i miss you\",\n",
    "    \"iow\": \"in other words\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"j4f\": \"just for fun\",\n",
    "    \"jic\": \"just in case\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"jsyk\": \"just so you know\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"lb\": \"pound\",\n",
    "    \"lbs\": \"pounds\",\n",
    "    \"ldr\": \"long distance relationship\",\n",
    "    \"lmao\": \"laugh my ass off\",\n",
    "    \"lmfao\": \"laugh my fucking ass off\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"ltd\": \"limited\",\n",
    "    \"ltns\": \"long time no see\",\n",
    "    \"m8\": \"mate\",\n",
    "    \"mf\": \"motherfucker\",\n",
    "    \"mfs\": \"motherfuckers\",\n",
    "    \"mfw\": \"my face when\",\n",
    "    \"mofo\": \"motherfucker\",\n",
    "    \"mph\": \"miles per hour\",\n",
    "    \"mr\": \"mister\",\n",
    "    \"mrw\": \"my reaction when\",\n",
    "    \"ms\": \"miss\",\n",
    "    \"mte\": \"my thoughts exactly\",\n",
    "    \"nagi\": \"not a good idea\",\n",
    "    \"nbc\": \"national broadcasting company\",\n",
    "    \"nbd\": \"not big deal\",\n",
    "    \"nfs\": \"not for sale\",\n",
    "    \"ngl\": \"not going to lie\",\n",
    "    \"nhs\": \"national health service\",\n",
    "    \"nrn\": \"no reply necessary\",\n",
    "    \"nsfl\": \"not safe for life\",\n",
    "    \"nsfw\": \"not safe for work\",\n",
    "    \"nth\": \"nice to have\",\n",
    "    \"nvr\": \"never\",\n",
    "    \"nyc\": \"new york city\",\n",
    "    \"oc\": \"original content\",\n",
    "    \"og\": \"original\",\n",
    "    \"ohp\": \"overhead projector\",\n",
    "    \"oic\": \"oh i see\",\n",
    "    \"omdb\": \"over my dead body\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"p.a\": \"per annum\",\n",
    "    \"p.m\": \"after midday\",\n",
    "    \"pm\": \"prime minister\",\n",
    "    \"poc\": \"people of color\",\n",
    "    \"pov\": \"point of view\",\n",
    "    \"pp\": \"pages\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"prw\": \"parents are watching\",\n",
    "    \"ps\": \"postscript\",\n",
    "    \"pt\": \"point\",\n",
    "    \"ptb\": \"please text back\",\n",
    "    \"pto\": \"please turn over\",\n",
    "    \"qpsa\": \"what happens\",  #\"que pasa\",\n",
    "    \"ratchet\": \"rude\",\n",
    "    \"rbtl\": \"read between the lines\",\n",
    "    \"rlrt\": \"real life retweet\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"roflol\": \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\": \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\": \"retweet\",\n",
    "    \"ruok\": \"are you ok\",\n",
    "    \"sfw\": \"safe for work\",\n",
    "    \"sk8\": \"skate\",\n",
    "    \"smh\": \"shake my head\",\n",
    "    \"sq\": \"square\",\n",
    "    \"srsly\": \"seriously\",\n",
    "    \"ssdd\": \"same stuff different day\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"tbs\": \"tablespooful\",\n",
    "    \"tbsp\": \"tablespooful\",\n",
    "    \"tfw\": \"that feeling when\",\n",
    "    \"thks\": \"thank you\",\n",
    "    \"tho\": \"though\",\n",
    "    \"thx\": \"thank you\",\n",
    "    \"tia\": \"thanks in advance\",\n",
    "    \"til\": \"today i learned\",\n",
    "    \"tl;dr\": \"too long i did not read\",\n",
    "    \"tldr\": \"too long i did not read\",\n",
    "    \"tmb\": \"tweet me back\",\n",
    "    \"tntl\": \"trying not to laugh\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"u\": \"you\",\n",
    "    \"u2\": \"you too\",\n",
    "    \"u4e\": \"yours for ever\",\n",
    "    \"utc\": \"coordinated universal time\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"w8\": \"wait\",\n",
    "    \"wassup\": \"what is up\",\n",
    "    \"wb\": \"welcome back\",\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"wtg\": \"way to go\",\n",
    "    \"wtpa\": \"where the party at\",\n",
    "    \"wuf\": \"where are you from\",\n",
    "    \"wuzup\": \"what is up\",\n",
    "    \"wywh\": \"wish you were here\",\n",
    "    \"yd\": \"yard\",\n",
    "    \"ygtr\": \"you got that right\",\n",
    "    \"ynk\": \"you never know\",\n",
    "    \"zzz\": \"sleeping bored and tired\"\n",
    "}\n",
    "\n",
    "\n",
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def convert_abbrev(word):\n",
    "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
    "\n",
    "\n",
    "# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "def convert_abbrev_in_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [convert_abbrev(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Lemmatization & stemming\n",
    "# Lemma\n",
    "def lemma(text):\n",
    "    txt1 = wordnet_lemmatizer.lemmatize(text, pos=wordnet.NOUN)\n",
    "    txt2 = wordnet_lemmatizer.lemmatize(text, pos=wordnet.VERB)\n",
    "    txt3 = wordnet_lemmatizer.lemmatize(text, pos=wordnet.ADJ)\n",
    "    if len(txt1) <= len(txt2) and len(txt1) <= len(txt3):\n",
    "        text = txt1\n",
    "    elif len(txt2) <= len(txt1) and len(txt2) <= len(txt3):\n",
    "        text = txt2\n",
    "    else:\n",
    "        text = txt3\n",
    "\n",
    "    return text‏‏\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()‏‏  #%%\n",
    "    import re\n",
    "    import string\n",
    "    # contractions_dict = {\n",
    "    #     \"ain't\": \"are not\",\n",
    "    #     \"aren't\": \"are not\",\n",
    "    #     \"can't\": \"cannot\",\n",
    "    #     \"couldn't\": \"could not\",\n",
    "    #     \"could've\": \"could have\",\n",
    "    #     \"didn't\": \"did not\",\n",
    "    #     \"doesn't\": \"does not\",\n",
    "    #     \"don't\": \"do not\",\n",
    "    #     \"hadn't\": \"had not\",\n",
    "    #     \"hasn't\": \"has not\",\n",
    "    #     \"haven't\": \"have not\",\n",
    "    #     \"he's\": \"he is\",\n",
    "    #     \"he'll\": \"he will\",\n",
    "    #     \"he'd\": \"he would\",\n",
    "    #     \"here's\": \"here is\",\n",
    "    #     \"how's\": \"how is\",\n",
    "    #     \"I'll\": \"I will\",\n",
    "    #     \"I'm\": \"I am\",\n",
    "    #     \"I've\": \"I have\",\n",
    "    #     \"isn't\": \"is not\",\n",
    "    #     \"it's\": \"it is\",\n",
    "    #     \"let's\": \"let us\",\n",
    "    #     \"ma'am\": \"madam\",\n",
    "    #     \"mightn't\": \"might not\",\n",
    "    #     \"mustn't\": \"must not\",\n",
    "    #     \"must've\": \"must have\",\n",
    "    #     \"needn't\": \"need not\",\n",
    "    #     \"shan't\": \"shall not\",\n",
    "    #     \"she's\": \"she is\",\n",
    "    #     \"shouldn't\": \"should not\",\n",
    "    #     \"should've\": \"should have\",\n",
    "    #     \"that's\": \"that is\",\n",
    "    #     \"there's\": \"there is\",\n",
    "    #     \"they're\": \"they are\",\n",
    "    #     \"they've\": \"they have\",\n",
    "    #     \"we're\": \"we are\",\n",
    "    #     \"we've\": \"we have\",\n",
    "    #     \"weren't\": \"were not\",\n",
    "    #     \"what's\": \"what is\",\n",
    "    #     \"who's\": \"who is\",\n",
    "    #     \"won't\": \"will not\",\n",
    "    #     \"wouldn't\": \"would not\",\n",
    "    #     \"you're\": \"you are\",\n",
    "    #     \"you've\": \"you have\",\n",
    "    # }\n",
    "\n",
    "    # contractions_dict = {\n",
    "    #     \"I'd\": \"I would\",\n",
    "    #     \"I'd've\": \"I would have\",\n",
    "    #     \"I'll\": \"I will\",\n",
    "    #     \"I'm\": \"I am\",\n",
    "    #     \"I've\": \"I have\",\n",
    "    #     \"I'ven't\": \"I have not\",\n",
    "    #     \"it'd\": \"it would\",\n",
    "    #     \"it'd've\": \"it would have\",\n",
    "    #     \"it'll\": \"it will\",\n",
    "    #     \"it's\": \"it is\",\n",
    "    #     \"let's\": \"let us\",\n",
    "    #     \"ma'am\": \"madam\",\n",
    "    #     \"mightn't\": \"might not\",\n",
    "    #     \"must've\": \"must have\",\n",
    "    #     \"needn't\": \"need not\",\n",
    "    #     \"o'clock\": \"of the clock\",\n",
    "    #     \"she'd\": \"she would\",\n",
    "    #     \"she'd've\": \"she would have\",\n",
    "    #     \"she'll\": \"she will\",\n",
    "    #     \"she's\": \"she is\",\n",
    "    #     \"should've\": \"should have\",\n",
    "    #     \"that's\": \"that is\",\n",
    "    #     \"there's\": \"there is\",\n",
    "    #     \"they'd\": \"they would\",\n",
    "    #     \"they'd've\": \"they would have\",\n",
    "    #     \"they'll\": \"they will\",\n",
    "    #     \"they're\": \"they are\",\n",
    "    #     \"they've\": \"they have\",\n",
    "    #     \"we'd\": \"we would\",\n",
    "    #     \"we'd've\": \"we would have\",\n",
    "    #     \"we'll\": \"we will\",\n",
    "    #     \"we're\": \"we are\",\n",
    "    #     \"we've\": \"we have\",\n",
    "    #     \"what'll\": \"what will\",\n",
    "    #     \"what're\": \"what are\",\n",
    "    #     \"what's\": \"what is\",\n",
    "    #     \"what've\": \"what have\",\n",
    "    #     \"where's\": \"where is\",\n",
    "    #     \"who'd\": \"who would\",\n",
    "    #     \"who'd've\": \"who would have\",\n",
    "    #     \"who'll\": \"who will\",\n",
    "    #     \"who're\": \"who are\",\n",
    "    #     \"who's\": \"who is\",\n",
    "    #     \"who've\": \"who have\",\n",
    "    #     \"won't\": \"will not\",\n",
    "    #     \"would've\": \"would have\",\n",
    "    #     \"y'all\": \"you all\",\n",
    "    #     \"you'd\": \"you would\",\n",
    "    #     \"you'd've\": \"you would have\",\n",
    "    #     \"you'll\": \"you will\",\n",
    "    #     \"you're\": \"you are\",\n",
    "    #     \"you've\": \"you have\",\n",
    "    # }\n",
    "    def expand_contractions(input_str):\n",
    "        input_str = re.sub(r\"he's\", \"he is\", input_str)\n",
    "        input_str = re.sub(r\"there's\", \"there is\", input_str)\n",
    "        input_str = re.sub(r\"We're\", \"We are\", input_str)\n",
    "        input_str = re.sub(r\"That's\", \"That is\", input_str)\n",
    "        input_str = re.sub(r\"won't\", \"will not\", input_str)\n",
    "        input_str = re.sub(r\"they're\", \"they are\", input_str)\n",
    "        input_str = re.sub(r\"Can't\", \"Cannot\", input_str)\n",
    "        input_str = re.sub(r\"wasn't\", \"was not\", input_str)\n",
    "        input_str = re.sub(r\"don\\x89Ûªt\", \"do not\", input_str)\n",
    "        input_str = re.sub(r\"aren't\", \"are not\", input_str)\n",
    "        input_str = re.sub(r\"isn't\", \"is not\", input_str)\n",
    "        input_str = re.sub(r\"What's\", \"What is\", input_str)\n",
    "        input_str = re.sub(r\"haven't\", \"have not\", input_str)\n",
    "        input_str = re.sub(r\"hasn't\", \"has not\", input_str)\n",
    "        input_str = re.sub(r\"There's\", \"There is\", input_str)\n",
    "        input_str = re.sub(r\"He's\", \"He is\", input_str)\n",
    "        input_str = re.sub(r\"It's\", \"It is\", input_str)\n",
    "        input_str = re.sub(r\"You're\", \"You are\", input_str)\n",
    "        input_str = re.sub(r\"I'M\", \"I am\", input_str)\n",
    "        input_str = re.sub(r\"shouldn't\", \"should not\", input_str)\n",
    "        input_str = re.sub(r\"wouldn't\", \"would not\", input_str)\n",
    "        input_str = re.sub(r\"i'm\", \"I am\", input_str)\n",
    "        input_str = re.sub(r\"I\\x89Ûªm\", \"I am\", input_str)\n",
    "        input_str = re.sub(r\"I'm\", \"I am\", input_str)\n",
    "        input_str = re.sub(r\"Isn't\", \"is not\", input_str)\n",
    "        input_str = re.sub(r\"Here's\", \"Here is\", input_str)\n",
    "        input_str = re.sub(r\"you've\", \"you have\", input_str)\n",
    "        input_str = re.sub(r\"you\\x89Ûªve\", \"you have\", input_str)\n",
    "        input_str = re.sub(r\"we're\", \"we are\", input_str)\n",
    "        input_str = re.sub(r\"what's\", \"what is\", input_str)\n",
    "        input_str = re.sub(r\"couldn't\", \"could not\", input_str)\n",
    "        input_str = re.sub(r\"we've\", \"we have\", input_str)\n",
    "        input_str = re.sub(r\"it\\x89Ûªs\", \"it is\", input_str)\n",
    "        input_str = re.sub(r\"doesn\\x89Ûªt\", \"does not\", input_str)\n",
    "        input_str = re.sub(r\"It\\x89Ûªs\", \"It is\", input_str)\n",
    "        input_str = re.sub(r\"Here\\x89Ûªs\", \"Here is\", input_str)\n",
    "        input_str = re.sub(r\"who's\", \"who is\", input_str)\n",
    "        input_str = re.sub(r\"I\\x89Ûªve\", \"I have\", input_str)\n",
    "        input_str = re.sub(r\"y'all\", \"you all\", input_str)\n",
    "        input_str = re.sub(r\"can\\x89Ûªt\", \"cannot\", input_str)\n",
    "        input_str = re.sub(r\"would've\", \"would have\", input_str)\n",
    "        input_str = re.sub(r\"it'll\", \"it will\", input_str)\n",
    "        input_str = re.sub(r\"we'll\", \"we will\", input_str)\n",
    "        input_str = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", input_str)\n",
    "        input_str = re.sub(r\"We've\", \"We have\", input_str)\n",
    "        input_str = re.sub(r\"he'll\", \"he will\", input_str)\n",
    "        input_str = re.sub(r\"Y'all\", \"You all\", input_str)\n",
    "        input_str = re.sub(r\"Weren't\", \"Were not\", input_str)\n",
    "        input_str = re.sub(r\"Didn't\", \"Did not\", input_str)\n",
    "        input_str = re.sub(r\"they'll\", \"they will\", input_str)\n",
    "        input_str = re.sub(r\"they'd\", \"they would\", input_str)\n",
    "        input_str = re.sub(r\"DON'T\", \"DO NOT\", input_str)\n",
    "        input_str = re.sub(r\"That\\x89Ûªs\", \"That is\", input_str)\n",
    "        input_str = re.sub(r\"they've\", \"they have\", input_str)\n",
    "        input_str = re.sub(r\"i'd\", \"I would\", input_str)\n",
    "        input_str = re.sub(r\"should've\", \"should have\", input_str)\n",
    "        input_str = re.sub(r\"You\\x89Ûªre\", \"You are\", input_str)\n",
    "        input_str = re.sub(r\"where's\", \"where is\", input_str)\n",
    "        input_str = re.sub(r\"Don\\x89Ûªt\", \"Do not\", input_str)\n",
    "        input_str = re.sub(r\"we'd\", \"we would\", input_str)\n",
    "        input_str = re.sub(r\"i'll\", \"I will\", input_str)\n",
    "        input_str = re.sub(r\"weren't\", \"were not\", input_str)\n",
    "        input_str = re.sub(r\"They're\", \"They are\", input_str)\n",
    "        input_str = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", input_str)\n",
    "        input_str = re.sub(r\"you\\x89Ûªll\", \"you will\", input_str)\n",
    "        input_str = re.sub(r\"I\\x89Ûªd\", \"I would\", input_str)\n",
    "        input_str = re.sub(r\"let's\", \"let us\", input_str)\n",
    "        input_str = re.sub(r\"it's\", \"it is\", input_str)\n",
    "        input_str = re.sub(r\"can't\", \"cannot\", input_str)\n",
    "        input_str = re.sub(r\"don't\", \"do not\", input_str)\n",
    "        input_str = re.sub(r\"you're\", \"you are\", input_str)\n",
    "        input_str = re.sub(r\"i've\", \"I have\", input_str)\n",
    "        input_str = re.sub(r\"that's\", \"that is\", input_str)\n",
    "        input_str = re.sub(r\"i'll\", \"I will\", input_str)\n",
    "        input_str = re.sub(r\"doesn't\", \"does not\", input_str)\n",
    "        input_str = re.sub(r\"i'd\", \"I would\", input_str)\n",
    "        input_str = re.sub(r\"didn't\", \"did not\", input_str)\n",
    "        input_str = re.sub(r\"ain't\", \"am not\", input_str)\n",
    "        input_str = re.sub(r\"you'll\", \"you will\", input_str)\n",
    "        input_str = re.sub(r\"I've\", \"I have\", input_str)\n",
    "        input_str = re.sub(r\"Don't\", \"do not\", input_str)\n",
    "        input_str = re.sub(r\"I'll\", \"I will\", input_str)\n",
    "        input_str = re.sub(r\"I'd\", \"I would\", input_str)\n",
    "        input_str = re.sub(r\"Let's\", \"Let us\", input_str)\n",
    "        input_str = re.sub(r\"you'd\", \"You would\", input_str)\n",
    "        input_str = re.sub(r\"It's\", \"It is\", input_str)\n",
    "        input_str = re.sub(r\"Ain't\", \"am not\", input_str)\n",
    "        input_str = re.sub(r\"Haven't\", \"Have not\", input_str)\n",
    "        input_str = re.sub(r\"Could've\", \"Could have\", input_str)\n",
    "        input_str = re.sub(r\"youve\", \"you have\", input_str)\n",
    "        input_str = re.sub(r\"donå«t\", \"do not\", input_str)\n",
    "\n",
    "        return input_str\n",
    "\n",
    "    def remove_urls(input_str):\n",
    "        input_str = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", input_str)\n",
    "        return input_str\n",
    "\n",
    "    def def(input_str):\n",
    "\n",
    "        return input_str\n",
    "\n",
    "    def separate_punctuations_from_words(input_str):\n",
    "        punctuations = string.punctuation\n",
    "        for p in punctuations:\n",
    "            input_str = input_str.replace(p, f' {p} ')\n",
    "        return input_str\n",
    "\n",
    "    def replace_continues_dots(input_str):\n",
    "        continues_dots = ['..', '...', '....', '.....', '......', '.......', '........', '.........', '..........', ]\n",
    "        for c in continues_dots:\n",
    "            input_str = input_str.replace(c, ' ... ')\n",
    "\n",
    "        return input_str\n",
    "\n",
    "    def remove_html_tags(input_str):\n",
    "        html = re.compile(r'<.*?>')\n",
    "        return html.sub(r'', input_str)\n",
    "\n",
    "    # df['text']=df['text'].apply(lambda x : remove_html(x))\n",
    "    # Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "    def remove_emoji(input_str):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', input_str)\n",
    "\n",
    "    # df['text']=df['text'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "    !pip install pyspellchecker\n",
    "    from spellchecker import SpellChecker\n",
    "    spell = SpellChecker()\n",
    "\n",
    "    def correct_spellings(input_str):\n",
    "        corrected_text = []\n",
    "        misspelled_words = spell.unknown(text.split())\n",
    "        for word in text.split():\n",
    "            if word in misspelled_words:\n",
    "                corrected_text.append(spell.correction(word))\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        return \" \".join(corrected_text)\n",
    "\n",
    "    text = \"corect me plese\"\n",
    "    correct_spellings(text)\n",
    "    glove_embeddings = np.load('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl',\n",
    "                               allow_pickle=True)\n",
    "    fasttext_embeddings = np.load('../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl',\n",
    "                                  allow_pickle=True)\n",
    "\n",
    "    def build_vocab(X):\n",
    "\n",
    "        tweets = X.apply(lambda s: s.split()).values\n",
    "        vocab = {}\n",
    "\n",
    "        for tweet in tweets:\n",
    "            for word in tweet:\n",
    "                try:\n",
    "                    vocab[word] += 1\n",
    "                except KeyError:\n",
    "                    vocab[word] = 1\n",
    "        return vocab\n",
    "\n",
    "    def check_embeddings_coverage(X, embeddings):\n",
    "\n",
    "        vocab = build_vocab(X)\n",
    "\n",
    "        covered = {}\n",
    "        oov = {}\n",
    "        n_covered = 0\n",
    "        n_oov = 0\n",
    "\n",
    "        for word in vocab:\n",
    "            try:\n",
    "                covered[word] = embeddings[word]\n",
    "                n_covered += vocab[word]\n",
    "            except:\n",
    "                oov[word] = vocab[word]\n",
    "                n_oov += vocab[word]\n",
    "\n",
    "        vocab_coverage = len(covered) / len(vocab)\n",
    "        text_coverage = (n_covered / (n_covered + n_oov))\n",
    "\n",
    "        sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "        return sorted_oov, vocab_coverage, text_coverage\n",
    "\n",
    "    train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(df_train['text'],\n",
    "                                                                                                       glove_embeddings)\n",
    "    test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(df_test['text'],\n",
    "                                                                                                    glove_embeddings)\n",
    "    print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(\n",
    "        train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "    print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage,\n",
    "                                                                                              test_glove_text_coverage))\n",
    "\n",
    "    train_fasttext_oov, train_fasttext_vocab_coverage, train_fasttext_text_coverage = check_embeddings_coverage(\n",
    "        df_train['text'], fasttext_embeddings)\n",
    "    test_fasttext_oov, test_fasttext_vocab_coverage, test_fasttext_text_coverage = check_embeddings_coverage(\n",
    "        df_test['text'], fasttext_embeddings)\n",
    "    print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(\n",
    "        train_fasttext_vocab_coverage, train_fasttext_text_coverage))\n",
    "    print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(\n",
    "        test_fasttext_vocab_coverage, test_fasttext_text_coverage))\n",
    "    # n_gram?\n",
    "    # maybe some common methods for showing stop words or etc?\n",
    "    # Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "    abbreviations = {\n",
    "        \"$\": \" dollar \",\n",
    "        \"€\": \" euro \",\n",
    "        \"4ao\": \"for adults only\",\n",
    "        \"a.m\": \"before midday\",\n",
    "        \"a3\": \"anytime anywhere anyplace\",\n",
    "        \"aamof\": \"as a matter of fact\",\n",
    "        \"acct\": \"account\",\n",
    "        \"adih\": \"another day in hell\",\n",
    "        \"afaic\": \"as far as i am concerned\",\n",
    "        \"afaict\": \"as far as i can tell\",\n",
    "        \"afaik\": \"as far as i know\",\n",
    "        \"afair\": \"as far as i remember\",\n",
    "        \"afk\": \"away from keyboard\",\n",
    "        \"app\": \"application\",\n",
    "        \"approx\": \"approximately\",\n",
    "        \"apps\": \"applications\",\n",
    "        \"asap\": \"as soon as possible\",\n",
    "        \"asl\": \"age, sex, location\",\n",
    "        \"atk\": \"at the keyboard\",\n",
    "        \"ave.\": \"avenue\",\n",
    "        \"aymm\": \"are you my mother\",\n",
    "        \"ayor\": \"at your own risk\",\n",
    "        \"b&b\": \"bed and breakfast\",\n",
    "        \"b+b\": \"bed and breakfast\",\n",
    "        \"b.c\": \"before christ\",\n",
    "        \"b2b\": \"business to business\",\n",
    "        \"b2c\": \"business to customer\",\n",
    "        \"b4\": \"before\",\n",
    "        \"b4n\": \"bye for now\",\n",
    "        \"b@u\": \"back at you\",\n",
    "        \"bae\": \"before anyone else\",\n",
    "        \"bak\": \"back at keyboard\",\n",
    "        \"bbbg\": \"bye bye be good\",\n",
    "        \"bbc\": \"british broadcasting corporation\",\n",
    "        \"bbias\": \"be back in a second\",\n",
    "        \"bbl\": \"be back later\",\n",
    "        \"bbs\": \"be back soon\",\n",
    "        \"be4\": \"before\",\n",
    "        \"bfn\": \"bye for now\",\n",
    "        \"blvd\": \"boulevard\",\n",
    "        \"bout\": \"about\",\n",
    "        \"brb\": \"be right back\",\n",
    "        \"bros\": \"brothers\",\n",
    "        \"brt\": \"be right there\",\n",
    "        \"bsaaw\": \"big smile and a wink\",\n",
    "        \"btw\": \"by the way\",\n",
    "        \"bwl\": \"bursting with laughter\",\n",
    "        \"c/o\": \"care of\",\n",
    "        \"cet\": \"central european time\",\n",
    "        \"cf\": \"compare\",\n",
    "        \"cia\": \"central intelligence agency\",\n",
    "        \"csl\": \"can not stop laughing\",\n",
    "        \"cu\": \"see you\",\n",
    "        \"cul8r\": \"see you later\",\n",
    "        \"cv\": \"curriculum vitae\",\n",
    "        \"cwot\": \"complete waste of time\",\n",
    "        \"cya\": \"see you\",\n",
    "        \"cyt\": \"see you tomorrow\",\n",
    "        \"dae\": \"does anyone else\",\n",
    "        \"dbmib\": \"do not bother me i am busy\",\n",
    "        \"diy\": \"do it yourself\",\n",
    "        \"dm\": \"direct message\",\n",
    "        \"dwh\": \"during work hours\",\n",
    "        \"e123\": \"easy as one two three\",\n",
    "        \"eet\": \"eastern european time\",\n",
    "        \"eg\": \"example\",\n",
    "        \"embm\": \"early morning business meeting\",\n",
    "        \"encl\": \"enclosed\",\n",
    "        \"encl.\": \"enclosed\",\n",
    "        \"etc\": \"and so on\",\n",
    "        \"faq\": \"frequently asked questions\",\n",
    "        \"fawc\": \"for anyone who cares\",\n",
    "        \"fb\": \"facebook\",\n",
    "        \"fc\": \"fingers crossed\",\n",
    "        \"fig\": \"figure\",\n",
    "        \"fimh\": \"forever in my heart\",\n",
    "        \"ft.\": \"feet\",\n",
    "        \"ft\": \"featuring\",\n",
    "        \"ftl\": \"for the loss\",\n",
    "        \"ftw\": \"for the win\",\n",
    "        \"fwiw\": \"for what it is worth\",\n",
    "        \"fyi\": \"for your information\",\n",
    "        \"g9\": \"genius\",\n",
    "        \"gahoy\": \"get a hold of yourself\",\n",
    "        \"gal\": \"get a life\",\n",
    "        \"gcse\": \"general certificate of secondary education\",\n",
    "        \"gfn\": \"gone for now\",\n",
    "        \"gg\": \"good game\",\n",
    "        \"gl\": \"good luck\",\n",
    "        \"glhf\": \"good luck have fun\",\n",
    "        \"gmt\": \"greenwich mean time\",\n",
    "        \"gmta\": \"great minds think alike\",\n",
    "        \"gn\": \"good night\",\n",
    "        \"g.o.a.t\": \"greatest of all time\",\n",
    "        \"goat\": \"greatest of all time\",\n",
    "        \"goi\": \"get over it\",\n",
    "        \"gps\": \"global positioning system\",\n",
    "        \"gr8\": \"great\",\n",
    "        \"gratz\": \"congratulations\",\n",
    "        \"gyal\": \"girl\",\n",
    "        \"h&c\": \"hot and cold\",\n",
    "        \"hp\": \"horsepower\",\n",
    "        \"hr\": \"hour\",\n",
    "        \"hrh\": \"his royal highness\",\n",
    "        \"ht\": \"height\",\n",
    "        \"ibrb\": \"i will be right back\",\n",
    "        \"ic\": \"i see\",\n",
    "        \"icq\": \"i seek you\",\n",
    "        \"icymi\": \"in case you missed it\",\n",
    "        \"idc\": \"i do not care\",\n",
    "        \"idgadf\": \"i do not give a damn fuck\",\n",
    "        \"idgaf\": \"i do not give a fuck\",\n",
    "        \"idk\": \"i do not know\",\n",
    "        \"ie\": \"that is\",\n",
    "        \"i.e\": \"that is\",\n",
    "        \"ifyp\": \"i feel your pain\",\n",
    "        \"IG\": \"instagram\",\n",
    "        \"iirc\": \"if i remember correctly\",\n",
    "        \"ilu\": \"i love you\",\n",
    "        \"ily\": \"i love you\",\n",
    "        \"imho\": \"in my humble opinion\",\n",
    "        \"imo\": \"in my opinion\",\n",
    "        \"imu\": \"i miss you\",\n",
    "        \"iow\": \"in other words\",\n",
    "        \"irl\": \"in real life\",\n",
    "        \"j4f\": \"just for fun\",\n",
    "        \"jic\": \"just in case\",\n",
    "        \"jk\": \"just kidding\",\n",
    "        \"jsyk\": \"just so you know\",\n",
    "        \"l8r\": \"later\",\n",
    "        \"lb\": \"pound\",\n",
    "        \"lbs\": \"pounds\",\n",
    "        \"ldr\": \"long distance relationship\",\n",
    "        \"lmao\": \"laugh my ass off\",\n",
    "        \"lmfao\": \"laugh my fucking ass off\",\n",
    "        \"lol\": \"laughing out loud\",\n",
    "        \"ltd\": \"limited\",\n",
    "        \"ltns\": \"long time no see\",\n",
    "        \"m8\": \"mate\",\n",
    "        \"mf\": \"motherfucker\",\n",
    "        \"mfs\": \"motherfuckers\",\n",
    "        \"mfw\": \"my face when\",\n",
    "        \"mofo\": \"motherfucker\",\n",
    "        \"mph\": \"miles per hour\",\n",
    "        \"mr\": \"mister\",\n",
    "        \"mrw\": \"my reaction when\",\n",
    "        \"ms\": \"miss\",\n",
    "        \"mte\": \"my thoughts exactly\",\n",
    "        \"nagi\": \"not a good idea\",\n",
    "        \"nbc\": \"national broadcasting company\",\n",
    "        \"nbd\": \"not big deal\",\n",
    "        \"nfs\": \"not for sale\",\n",
    "        \"ngl\": \"not going to lie\",\n",
    "        \"nhs\": \"national health service\",\n",
    "        \"nrn\": \"no reply necessary\",\n",
    "        \"nsfl\": \"not safe for life\",\n",
    "        \"nsfw\": \"not safe for work\",\n",
    "        \"nth\": \"nice to have\",\n",
    "        \"nvr\": \"never\",\n",
    "        \"nyc\": \"new york city\",\n",
    "        \"oc\": \"original content\",\n",
    "        \"og\": \"original\",\n",
    "        \"ohp\": \"overhead projector\",\n",
    "        \"oic\": \"oh i see\",\n",
    "        \"omdb\": \"over my dead body\",\n",
    "        \"omg\": \"oh my god\",\n",
    "        \"omw\": \"on my way\",\n",
    "        \"p.a\": \"per annum\",\n",
    "        \"p.m\": \"after midday\",\n",
    "        \"pm\": \"prime minister\",\n",
    "        \"poc\": \"people of color\",\n",
    "        \"pov\": \"point of view\",\n",
    "        \"pp\": \"pages\",\n",
    "        \"ppl\": \"people\",\n",
    "        \"prw\": \"parents are watching\",\n",
    "        \"ps\": \"postscript\",\n",
    "        \"pt\": \"point\",\n",
    "        \"ptb\": \"please text back\",\n",
    "        \"pto\": \"please turn over\",\n",
    "        \"qpsa\": \"what happens\",  #\"que pasa\",\n",
    "        \"ratchet\": \"rude\",\n",
    "        \"rbtl\": \"read between the lines\",\n",
    "        \"rlrt\": \"real life retweet\",\n",
    "        \"rofl\": \"rolling on the floor laughing\",\n",
    "        \"roflol\": \"rolling on the floor laughing out loud\",\n",
    "        \"rotflmao\": \"rolling on the floor laughing my ass off\",\n",
    "        \"rt\": \"retweet\",\n",
    "        \"ruok\": \"are you ok\",\n",
    "        \"sfw\": \"safe for work\",\n",
    "        \"sk8\": \"skate\",\n",
    "        \"smh\": \"shake my head\",\n",
    "        \"sq\": \"square\",\n",
    "        \"srsly\": \"seriously\",\n",
    "        \"ssdd\": \"same stuff different day\",\n",
    "        \"tbh\": \"to be honest\",\n",
    "        \"tbs\": \"tablespooful\",\n",
    "        \"tbsp\": \"tablespooful\",\n",
    "        \"tfw\": \"that feeling when\",\n",
    "        \"thks\": \"thank you\",\n",
    "        \"tho\": \"though\",\n",
    "        \"thx\": \"thank you\",\n",
    "        \"tia\": \"thanks in advance\",\n",
    "        \"til\": \"today i learned\",\n",
    "        \"tl;dr\": \"too long i did not read\",\n",
    "        \"tldr\": \"too long i did not read\",\n",
    "        \"tmb\": \"tweet me back\",\n",
    "        \"tntl\": \"trying not to laugh\",\n",
    "        \"ttyl\": \"talk to you later\",\n",
    "        \"u\": \"you\",\n",
    "        \"u2\": \"you too\",\n",
    "        \"u4e\": \"yours for ever\",\n",
    "        \"utc\": \"coordinated universal time\",\n",
    "        \"w/\": \"with\",\n",
    "        \"w/o\": \"without\",\n",
    "        \"w8\": \"wait\",\n",
    "        \"wassup\": \"what is up\",\n",
    "        \"wb\": \"welcome back\",\n",
    "        \"wtf\": \"what the fuck\",\n",
    "        \"wtg\": \"way to go\",\n",
    "        \"wtpa\": \"where the party at\",\n",
    "        \"wuf\": \"where are you from\",\n",
    "        \"wuzup\": \"what is up\",\n",
    "        \"wywh\": \"wish you were here\",\n",
    "        \"yd\": \"yard\",\n",
    "        \"ygtr\": \"you got that right\",\n",
    "        \"ynk\": \"you never know\",\n",
    "        \"zzz\": \"sleeping bored and tired\"\n",
    "    }\n",
    "\n",
    "    # Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "    def convert_abbrev(word):\n",
    "        return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
    "\n",
    "    # Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n",
    "    def convert_abbrev_in_text(text):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [convert_abbrev(word) for word in tokens]\n",
    "        text = ' '.join(tokens)\n",
    "        return text\n",
    "\n",
    "    # Lemmatization & stemming\n",
    "    # Lemma\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    # wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # def lemma(text):\n",
    "    #     txt1 = wordnet_lemmatizer.lemmatize(text,pos=wordnet.NOUN)\n",
    "    #     txt2 = wordnet_lemmatizer.lemmatize(text,pos=wordnet.VERB)\n",
    "    #     txt3 = wordnet_lemmatizer.lemmatize(text,pos=wordnet.ADJ)\n",
    "    #     if len(txt1) <= len(txt2) and len(txt1) <= len(txt3):\n",
    "    #         text = txt1\n",
    "    #     elif len(txt2) <= len(txt1) and len(txt2) <= len(txt3):\n",
    "    #         text = txt2\n",
    "    #     else:\n",
    "    #         text = txt3\n",
    "    #     return text\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # def lemma(text):\n",
    "    #     word_net_lemma = WordNetLemmatizer()\n",
    "    #\n",
    "    #     lemma_forms = [wordnet_lemmatizer.lemmatize(text, pos=wordnet.NOUN),\n",
    "    #                         wordnet_lemmatizer.lemmatize(text, pos=wordnet.VERB),\n",
    "    #                         wordnet_lemmatizer.lemmatize(text, pos=wordnet.ADJ)]\n",
    "    #\n",
    "    #     # Choose the first non-identical lemmatized form (preferably shorter)\n",
    "    #     chosen_form = next((form for form in lemmatized_forms if form != text), text)\n",
    "    #     return chosen_form\n",
    "\n",
    "    def lemma(text):\n",
    "        word_net_lemma = WordNetLemmatizer()\n",
    "        return word_net_lemma.lemmatize(text)\n",
    "\n",
    "    # Lemma\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "\n",
    "    def lemma(text):\n",
    "        word_net_lemma = WordNetLemmatizer()\n",
    "        return word_net_lemma.lemmatize(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
