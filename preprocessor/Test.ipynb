{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-11-11T17:57:21.245220Z",
     "end_time": "2023-11-11T17:57:21.254743Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install autocorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the striped bat be hang on their foot for best\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Preprocessor import Preprocessor\n",
    "text_processor = Preprocessor()\n",
    "input_text = \"The stripeddd bats are hanging on their feet for best\"\n",
    "# flags = ['contractions', 'urls', 'punctuation', 'spelling']\n",
    "processed_text = text_processor.process_text(input_text, contractions=True, urls=True,\n",
    "                                             abbreviations=True,\n",
    "                                             spelling=True,\n",
    "                                             # dots=True,\n",
    "                                             punctuation=True,\n",
    "                                             lowercase=True,\n",
    "                                             lemma=True,\n",
    "                                             html_tags=True\n",
    "                                             )\n",
    "print(processed_text)\n",
    "# print(text_processor.convert_abbrev_in_text(input_text))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T17:58:06.499625Z",
     "end_time": "2023-11-11T17:58:06.633612Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: C:\\Users\\Hooman\\AppData\\Roaming\\jupyter\\runtime\\kernel-d0203c14-1680-4091-a455-0da61bc338d6 (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute 'C:\\Users\\Hooman\\AppData\\Roaming\\jupyter\\runtime\\kernel-d0203c14-1680-4091-a455-0da61bc338d6'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from Preprocessor import Preprocessor\n",
    " # Replace with your actual module name\n",
    "\n",
    "class TestPreprocessor(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.preprocessor = Preprocessor()\n",
    "\n",
    "    def test_remove_urls(self):\n",
    "        input_text = \"Visit our website at https://www.example.com\"\n",
    "        expected_output = \"Visit our website at \"\n",
    "        print(input_text)\n",
    "        self.assertEqual(self.preprocessor.remove_urls(input_text), expected_output)\n",
    "    #\n",
    "    # def test_remove_html_tags(self):\n",
    "    #     input_text = \"<p>This is a <b>bold</b> statement.</p>\"\n",
    "    #     expected_output = \"This is a bold statement.\"\n",
    "    #     self.assertEqual(self.preprocessor.remove_html_tags(input_text), expected_output)\n",
    "    #\n",
    "    # def test_remove_emoji(self):\n",
    "    #     input_text = \"I love Python! üòçüêç\"\n",
    "    #     expected_output = \"I love Python! \"\n",
    "    #     self.assertEqual(self.preprocessor.remove_emoji(input_text), expected_output)\n",
    "    #\n",
    "    # def test_separate_punctuations_from_words(self):\n",
    "    #     input_text = \"Hello, how are you?\"\n",
    "    #     expected_output = \"Hello ,  how are you ? \"\n",
    "    #     self.assertEqual(\n",
    "    #         self.preprocessor.separate_punctuations_from_words(input_text), expected_output\n",
    "    #     )\n",
    "    #\n",
    "    # def test_autocorrect_text(self):\n",
    "    #     input_text = \"Speling misteaks can be embarassing.\"\n",
    "    #     expected_output = \"Spelling mistakes can be embarrassing.\"\n",
    "    #     self.assertEqual(self.preprocessor.autocorrect_text(input_text), expected_output)\n",
    "    #\n",
    "    # def test_convert_abbrev_in_text(self):\n",
    "    #     input_text = \"btw, lmao, atm\"\n",
    "    #     expected_output = \"by the way ,  laughing my ass off ,  at the moment\"\n",
    "    #     self.assertEqual(\n",
    "    #         self.preprocessor.convert_abbrev_in_text(input_text), expected_output\n",
    "    #     )\n",
    "    #\n",
    "    # def test_lemma(self):\n",
    "    #     input_text = \"running\"\n",
    "    #     expected_output = \"run\"\n",
    "    #     self.assertEqual(self.preprocessor.lemma(input_text), expected_output)\n",
    "    #\n",
    "    # def test_to_lowercase(self):\n",
    "    #     input_text = \"Convert This Text To Lowercase\"\n",
    "    #     expected_output = \"convert this text to lowercase\"\n",
    "    #     self.assertEqual(self.preprocessor.to_lowercase(input_text), expected_output)\n",
    "    #\n",
    "    # def test_process_text(self):\n",
    "    #     input_text = \"Hello, World! Visit https://www.example.com for more info.\"\n",
    "    #     expected_output = \"hello ,  world !  visit  for more info . \"\n",
    "    #     self.assertEqual(\n",
    "    #         self.preprocessor.process_text(\n",
    "    #             input_text, urls=True, punctuation=True, lowercase=True\n",
    "    #         ),\n",
    "    #         expected_output,\n",
    "    #     )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-07T19:38:21.066813Z",
     "end_time": "2023-11-07T19:38:21.082825Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from Preprocessor import Preprocessor\n",
    "test = TestPreprocessor()\n",
    "test.setUp()\n",
    "test.test_remove_urls()\n",
    "# input_text = \"I've luve Python prograig....... Visitt viisting </> https://example.com for more info.\"\n",
    "# # flags = ['contractions', 'urls', 'punctuation', 'spelling']\n",
    "# processed_text = text_processor.process_text(input_text, contractions=True, urls=True,\n",
    "#                                              spelling=True,\n",
    "#                                              # dots=True,\n",
    "#                                              punctuation=True,\n",
    "#                                              lowercase=True,\n",
    "#                                              lemma=True,\n",
    "#                                              html_tags=True\n",
    "#                                              )\n",
    "# print(processed_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T17:08:00.685572Z",
     "end_time": "2023-11-11T17:08:00.745143Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%run TestPreprocessor.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\faran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\faran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\faran\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T13:14:38.482372200Z",
     "start_time": "2023-11-12T13:13:59.275007600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "     ---------------------------------------- 0.0/622.8 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/622.8 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/622.8 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/622.8 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/622.8 kB 119.1 kB/s eta 0:00:05\n",
      "     -- ---------------------------------- 41.0/622.8 kB 140.3 kB/s eta 0:00:05\n",
      "     -- ---------------------------------- 41.0/622.8 kB 140.3 kB/s eta 0:00:05\n",
      "     --- --------------------------------- 61.4/622.8 kB 182.2 kB/s eta 0:00:04\n",
      "     ---- -------------------------------- 71.7/622.8 kB 178.6 kB/s eta 0:00:04\n",
      "     ---- -------------------------------- 71.7/622.8 kB 178.6 kB/s eta 0:00:04\n",
      "     ------ ----------------------------- 112.6/622.8 kB 226.0 kB/s eta 0:00:03\n",
      "     ------- ---------------------------- 122.9/622.8 kB 218.3 kB/s eta 0:00:03\n",
      "     ------- ---------------------------- 122.9/622.8 kB 218.3 kB/s eta 0:00:03\n",
      "     -------- --------------------------- 143.4/622.8 kB 236.9 kB/s eta 0:00:03\n",
      "     -------- --------------------------- 153.6/622.8 kB 229.4 kB/s eta 0:00:03\n",
      "     ---------- ------------------------- 174.1/622.8 kB 233.1 kB/s eta 0:00:02\n",
      "     ----------- ------------------------ 194.6/622.8 kB 251.1 kB/s eta 0:00:02\n",
      "     ----------- ------------------------ 194.6/622.8 kB 251.1 kB/s eta 0:00:02\n",
      "     ------------- ---------------------- 225.3/622.8 kB 254.8 kB/s eta 0:00:02\n",
      "     ------------- ---------------------- 235.5/622.8 kB 248.7 kB/s eta 0:00:02\n",
      "     ------------- ---------------------- 235.5/622.8 kB 248.7 kB/s eta 0:00:02\n",
      "     -------------- --------------------- 256.0/622.8 kB 257.9 kB/s eta 0:00:02\n",
      "     --------------- -------------------- 276.5/622.8 kB 266.2 kB/s eta 0:00:02\n",
      "     ---------------- ------------------- 286.7/622.8 kB 260.3 kB/s eta 0:00:02\n",
      "     ----------------- ------------------ 307.2/622.8 kB 264.0 kB/s eta 0:00:02\n",
      "     ------------------ ----------------- 317.4/622.8 kB 262.1 kB/s eta 0:00:02\n",
      "     ------------------- ---------------- 337.9/622.8 kB 265.5 kB/s eta 0:00:02\n",
      "     -------------------- --------------- 358.4/622.8 kB 271.8 kB/s eta 0:00:01\n",
      "     -------------------- --------------- 358.4/622.8 kB 271.8 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 368.6/622.8 kB 266.7 kB/s eta 0:00:01\n",
      "     ---------------------- ------------- 389.1/622.8 kB 269.5 kB/s eta 0:00:01\n",
      "     ----------------------- ------------ 399.4/622.8 kB 267.8 kB/s eta 0:00:01\n",
      "     ----------------------- ------------ 409.6/622.8 kB 269.1 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 430.1/622.8 kB 274.2 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 430.1/622.8 kB 274.2 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 430.1/622.8 kB 274.2 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 430.1/622.8 kB 274.2 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 450.6/622.8 kB 253.9 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 460.8/622.8 kB 253.0 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 481.3/622.8 kB 257.7 kB/s eta 0:00:01\n",
      "     --------------------------- -------- 481.3/622.8 kB 257.7 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 512.0/622.8 kB 259.0 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 532.5/622.8 kB 261.1 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 532.5/622.8 kB 261.1 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 542.7/622.8 kB 258.2 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 563.2/622.8 kB 258.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 573.4/622.8 kB 259.3 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 593.9/622.8 kB 263.0 kB/s eta 0:00:01\n",
      "     -----------------------------------  614.4/622.8 kB 264.9 kB/s eta 0:00:01\n",
      "     ------------------------------------ 622.8/622.8 kB 264.9 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py): started\n",
      "  Building wheel for autocorrect (setup.py): finished with status 'done'\n",
      "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622374 sha256=bbaf8c7a7fef249fbfad9f36733dba416e011fffcdc61836e56e76984bb133de\n",
      "  Stored in directory: c:\\users\\faran\\appdata\\local\\pip\\cache\\wheels\\5e\\90\\99\\807a5ad861ce5d22c3c299a11df8cba9f31524f23ae6e645cb\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T12:37:31.377613900Z",
     "start_time": "2023-11-12T12:37:18.545197200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# test on the dataset\n",
    "from preprocessor.Preprocessor import Preprocessor\n",
    "import pandas as pd\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "\n",
    "df_train = pd.read_csv(\"../dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T12:37:43.241805Z",
     "start_time": "2023-11-12T12:37:43.125128200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "df_train20 = df_train[0:20]\n",
    "preprocessing_options = {\n",
    "    'lowercase': True,\n",
    "    'contractions': True,\n",
    "    'urls': True,\n",
    "    'punctuation': True,\n",
    "    'html_tags': True,\n",
    "    'emoji': True,\n",
    "    'spelling': True,\n",
    "    'abbreviations': True,\n",
    "    'lemma': True\n",
    "}\n",
    "\n",
    "\n",
    "df_train20_text = df_train20['text'].apply(lambda x: preprocessor.process_text(x, **preprocessing_options))\n",
    "# df_train20_location = df_train20['location'].apply(lambda x: preprocessor.process_text(x, **preprocessing_options))\n",
    "\n",
    "# df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T13:14:45.997830400Z",
     "start_time": "2023-11-12T13:14:44.217840100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 5) (20,)\n",
      "[['- Our', '+ our', '- Deeds', '? ^\\n', '+ deeds', '? ^\\n', '- are', '+ be', '  the', '- Reason', '? ^\\n', '+ reason', '? ^\\n', '  of', '  this', '- #earthquake', '? -\\n', '+ earthquake', '- May', '- ALLAH', '+ may', '+ alla', '- Forgive', '? ^\\n', '+ forgive', '? ^\\n', '- us', '+ u', '  all'], ['- Forest', '? ^\\n', '+ forest', '? ^\\n', '  fire', '  near', '- La', '- Ronge', '- Sask.', '+ la', '+ range', '+ task', '+ .', '- Canada', '? ^\\n', '+ canada', '? ^\\n'], ['- All', '+ all', '- residents', '?         -\\n', '+ resident', '- asked', '?    --\\n', '+ ask', '  to', \"+ '\", \"- 'shelter\", '? -\\n', '+ shelter', '  in', \"- place'\", '?      -\\n', '+ place', '- are', '- being', '- notified', \"+ '\", '+ be', '+ be', '+ notify', '  by', '- officers.', '?        --\\n', '+ officer', '- No', '+ .', '+ no', '  other', '  evacuation', '  or', '  shelter', '  in', '  place', '- orders', '?      -\\n', '+ order', '- are', '+ be', '- expected', '?       --\\n', '+ expect'], ['- 13,000', '+ 13', '+ ,', '+ 000', '  people', '  receive', '- #wildfires', '? -        -\\n', '+ wildfire', '  evacuation', '- orders', '?      -\\n', '+ order', '  in', '- California', '? ^\\n', '+ california', '? ^\\n'], ['- Just', '? ^\\n', '+ just', '? ^\\n', '- got', '+ get', '- sent', '?    ^\\n', '+ send', '?    ^\\n', '  this', '  photo', '  from', '- Ruby', '? ^\\n', '+ ruby', '? ^\\n', '- #Alaska', '? ^^\\n', '+ alaska', '? ^\\n', '- as', '+ a', '  smoke', '  from', '- #wildfires', '? -        -\\n', '+ wildfire', '- pours', '?     -\\n', '+ pour', '  into', '  a', '  school'], ['- #RockyFire', '+ rockyfire', '- Update', '? ^\\n', '+ update', '? ^\\n', '- =>', '+ =', '+ >', '- California', '? ^\\n', '+ california', '? ^\\n', '- Hwy.', '+ hwy', '+ .', '  20', '- closed', '?      -\\n', '+ close', '  in', '  both', '- directions', '?          -\\n', '+ direction', '  due', '  to', '- Lake', '? ^\\n', '+ lake', '? ^\\n', '- County', '? ^\\n', '+ county', '? ^\\n', '  fire', '  -', '- #CAfire', '+ fire', '- #wildfires', '? -        -\\n', '+ wildfire'], ['- #flood', '? -\\n', '+ flood', '- #disaster', '? -\\n', '+ disaster', '- Heavy', '? ^\\n', '+ heavy', '? ^\\n', '  rain', '- causes', '?      -\\n', '+ cause', '  flash', '  flooding', '  of', '- streets', '?       -\\n', '+ street', '  in', '- Manitou,', '+ monitor', '+ ,', '- Colorado', '? ^\\n', '+ colorado', '? ^\\n', '- Springs', '? ^\\n', '+ springs', '? ^\\n', '- areas', '?     -\\n', '+ area'], [\"- I'm\", '+ i', \"+ '\", '+ m', '  on', '  top', '  of', '  the', '  hill', '  and', '- I', '+ i', '  can', '  see', '  a', '  fire', '  in', '  the', '- woods...', '+ wood', '+ .', '+ .', '+ .'], [\"- There's\", '+ there', '+ be', '  an', '  emergency', '  evacuation', '- happening', '?       ---\\n', '+ happen', '  now', '  in', '  the', '  building', '  across', '  the', '  street'], [\"- I'm\", '+ i', \"+ '\", '+ m', '  afraid', '  that', '  the', '  tornado', '- is', '- coming', '+ be', '+ come', '  to', '  our', '- area...', '+ area', '+ .', '+ .', '+ .'], ['- Three', '? ^\\n', '+ three', '? ^\\n', '  people', '- died', '?    -\\n', '+ die', '  from', '  the', '  heat', '  wave', '  so', '  far'], ['- Haha', '? ^\\n', '+ haha', '? ^\\n', '- South', '? ^\\n', '+ south', '? ^\\n', '- Tampa', '? ^\\n', '+ tampa', '? ^\\n', '- is', '- getting', '+ be', '+ get', '- flooded', '?      --\\n', '+ flood', '- hah-', '- WAIT', '- A', '- SECOND', '- I', '- LIVE', '- IN', '- SOUTH', '- TAMPA', '- WHAT', '- AM', '- I', '- GONNA', '- DO', '- WHAT', '- AM', '- I', '- GONNA', '- DO', '- FVCK', '+ has', '+ -', '+ wait', '+ a', '+ second', '+ i', '+ live', '+ in', '+ south', '+ tampa', '+ what', '+ am', '+ i', '+ gon', '+ na', '+ do', '+ what', '+ am', '+ i', '+ gon', '+ na', '+ do', '+ fuck', '- #flooding', '? -\\n', '+ flooding'], ['- #raining', '- #flooding', '+ rain', '+ flood', '- #Florida', '? ^^\\n', '+ florida', '? ^\\n', '- #TampaBay', '- #Tampa', '+ tampabay', '+ tampa', '  18', '  or', '  19', '- days.', '?    --\\n', '+ day', \"- I've\", '+ .', '+ i', '+ have', '- lost', '?    ^\\n', '+ lose', '?    ^\\n', '  count'], ['- #Flood', '+ flood', '  in', '- Bago', '? -\\n', '+ ago', '- Myanmar', '? ^\\n', '+ myanmar', '? ^\\n', '- #We', '+ we', '- arrived', '?       -\\n', '+ arrive', '- Bago', '? -\\n', '+ ago'], ['- Damage', '? ^\\n', '+ damage', '? ^\\n', '  to', '  school', '  bus', '  on', '  80', '  in', '  multi', '  car', '  crash', '- #BREAKING', '+ breaking'], [\"- What's\", '+ what', '+ be', '  up', '- man?', '?    -\\n', '+ man', '+ ?'], ['- I', '+ i', '  love', '- fruits', '?      -\\n', '+ fruit'], ['- Summer', '? ^\\n', '+ summer', '? ^\\n', '- is', '+ be', '  lovely'], ['- My', '+ my', '  car', '- is', '+ be', '  so', '  fast'], ['- What', '? ^\\n', '+ what', '? ^\\n', '  a', '- goooooooaaaaaal!!!!!!', '?                ------\\n', '+ goooooooaaaaaal', '+ !', '+ !', '+ !', '+ !', '+ !', '+ !']]\n"
     ]
    }
   ],
   "source": [
    "# import difflib\n",
    "# differences = []\n",
    "# print(df_train20.shape, df_train20_text.shape)\n",
    "# for i, row in df_train20.iterrows():\n",
    "#     text1 = row['text']\n",
    "#\n",
    "#     text2 = df_train20_text.loc[i]\n",
    "#\n",
    "#     d = difflib.Differ()\n",
    "#     diff = list(d.compare(text1.split(), text2.split()))\n",
    "#     differences.append(diff)\n",
    "#     # print(f\"Row {i + 1} differences: {' '.join(diff)}\")\n",
    "#\n",
    "# print(differences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T13:18:50.256632Z",
     "start_time": "2023-11-12T13:18:50.249593600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# df_concatenated = pd.concat([df_train20['text'], df_train20_text], axis=0, ignore_index=True)\n",
    "# df_result = pd.DataFrame({'concatenated_text': df_concatenated})\n",
    "df_result = pd.DataFrame({'text_df1': df_train20['text'], 'text_df2': df_train20_text})\n",
    "df_result.to_csv('df_concatenated.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T13:23:56.808176500Z",
     "start_time": "2023-11-12T13:23:56.798119200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # def separate_punctuations_from_words(self, text):\n",
    "#     continues_dots = ['.  .  .  .  .  .  .  .', '.  .  .  .  .  .  .', '.  .  .  .  .  .',\n",
    "#                       '.  .  .  .  .  .', '.  .  .  .  .', '.  .  .  .', '.  .  .', '.  .']\n",
    "#     remove_punctuation_list = ['#', '@', '¬â√ª√Ø', '¬â√ª√≤', '¬â√õ√í', '¬â',]  # can be added to its list\n",
    "#     for p in string.punctuation:\n",
    "#         if p in remove_punctuation_list:\n",
    "#             text = re.sub(p, '', text)\n",
    "#         else:\n",
    "#             text = text.replace(p, f' {p} ')\n",
    "#     for c in continues_dots:\n",
    "#         text = text.replace(c, ' ... ')\n",
    "#     return text\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller\n",
    "\n",
    "\n",
    "class Preprocessor1:\n",
    "    def __init__(self):\n",
    "        self.load_contractions()\n",
    "        self.load_abbreviations()\n",
    "        self.spell = Speller()\n",
    "\n",
    "    def load_contractions(self):\n",
    "        filepath = \"resources/contractions.json\"\n",
    "        with open(filepath, \"r\") as json_file:\n",
    "            self.contractions = json.load(json_file)\n",
    "\n",
    "    def load_abbreviations(self):\n",
    "        filepath = \"resources/abbreviations.json\"\n",
    "        with open(filepath, \"r\") as json_file:\n",
    "            self.abbreviations = json.load(json_file)\n",
    "\n",
    "    # def load_contractions(self):\n",
    "    #     with open(\"resources/contractions.json\", \"r\") as json_file:\n",
    "    #         self.contractions = json.load(json_file)\n",
    "    #\n",
    "    # def load_abbreviations(self):\n",
    "    #     with open(\"resources/abbreviations.json\", \"r\") as json_file:\n",
    "    #         self.abbreviations = json.load(json_file)\n",
    "\n",
    "    def replace_contractions(self, text):\n",
    "        for key, value in self.contractions.items():\n",
    "            text = re.sub(re.escape(key), value, text)\n",
    "        return text\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'https?://\\S+|www\\.\\S+|ftp://\\S+', '', text)\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        return re.compile(r'<.*?>').sub('', text)\n",
    "\n",
    "    def remove_emoji(self, text):\n",
    "        emoji_pattern = re.compile('['\n",
    "                                   u'\\U0001F600-\\U0001F64F'\n",
    "                                   u'\\U0001F300-\\U0001F5FF'\n",
    "                                   u'\\U0001F680-\\U0001F6FF'\n",
    "                                   u'\\U0001F1E0-\\U0001F1FF'\n",
    "                                   u'\\U00002702-\\U000027B0'\n",
    "                                   u'\\U000024C2-\\U0001F251'\n",
    "                                   ']+', flags=re.UNICODE)\n",
    "        return emoji_pattern.sub('', text)\n",
    "\n",
    "    def separate_punctuations_from_words(self, text):\n",
    "        continues_dots = ['.  .  .  .  .  .  .  .', '.  .  .  .  .  .  .', '.  .  .  .  .  .',\n",
    "                          '.  .  .  .  .  .', '.  .  .  .  .', '.  .  .  .', '.  .  .', '.  .']\n",
    "        remove_punctuation_list = ['#', '@', '¬â√ª√Ø',]  # can be added to its list\n",
    "        for p in string.punctuation:\n",
    "            if p in remove_punctuation_list:\n",
    "                text = re.sub(p, '', text)\n",
    "            else:\n",
    "                text = text.replace(p, f' {p} ')\n",
    "        for c in continues_dots:\n",
    "            text = text.replace(c, ' ... ')\n",
    "        return text\n",
    "\n",
    "    def autocorrect_text(self, input_text):\n",
    "        # !pip install autocorrect\n",
    "\n",
    "        words = input_text.split()\n",
    "        corrected_words = []\n",
    "\n",
    "        for word in words:\n",
    "            corrected_word = self.spell(word)\n",
    "            if corrected_word != word:\n",
    "                corrected_words.append(corrected_word)\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "\n",
    "        corrected_text = \" \".join(corrected_words)\n",
    "        return corrected_text\n",
    "\n",
    "    def convert_abbrev_in_text(self, text):\n",
    "        words = text.split()\n",
    "        converted_words = [self.abbreviations.get(word.lower(), word) for word in words]\n",
    "        converted_text = ' '.join(converted_words)\n",
    "        return converted_text\n",
    "\n",
    "    def lemma(self, text):\n",
    "        # nltk.download('punkt')\n",
    "        # nltk.download('averaged_perceptron_tagger')\n",
    "        # nltk.download('wordnet')\n",
    "\n",
    "        word_net_lemma = WordNetLemmatizer()\n",
    "\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "\n",
    "        pos_tags = [(token, self.get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
    "\n",
    "        lemmatized_tokens = [word_net_lemma.lemmatize(token, pos=tag) for token, tag in pos_tags]\n",
    "\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "\n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def to_lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def process_text(self, input_text,\n",
    "                     urls=True,\n",
    "                     punctuation=True,\n",
    "                     abbreviations=True,\n",
    "                     html_tags=True,\n",
    "                     emoji=True,\n",
    "                     lowercase=True,\n",
    "                     contractions=True,\n",
    "\n",
    "                     spelling=False,\n",
    "                     lemma=False\n",
    "                     ):\n",
    "        processed_text = input_text\n",
    "\n",
    "        # remove\n",
    "        if urls:\n",
    "            processed_text = self.remove_urls(processed_text)\n",
    "        if html_tags:\n",
    "            processed_text = self.remove_html_tags(processed_text)\n",
    "        if emoji:\n",
    "            processed_text = self.remove_emoji(processed_text)\n",
    "\n",
    "        # replace\n",
    "        if lowercase:\n",
    "            processed_text = self.to_lowercase(processed_text)\n",
    "        if abbreviations:  # lmao\n",
    "            processed_text = self.convert_abbrev_in_text(processed_text)\n",
    "        if contractions:  # It's\n",
    "            processed_text = self.replace_contractions(processed_text)\n",
    "        if punctuation:  # good.so -> good . so\n",
    "            processed_text = self.separate_punctuations_from_words(processed_text)\n",
    "        if lemma:\n",
    "            processed_text = self.lemma(processed_text)\n",
    "        if spelling:\n",
    "            processed_text = self.autocorrect_text(processed_text)\n",
    "\n",
    "        return processed_text\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T13:47:03.060674600Z",
     "start_time": "2023-11-12T13:47:03.057943800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im afraid that the tornado be come to our area ...\n"
     ]
    }
   ],
   "source": [
    "text_temp = 'I\\'m afraid that the tornado is coming to our area...'\n",
    "text_processor = Preprocessor1()\n",
    "processed_text = text_processor.process_text(text_temp,\n",
    "                                             spelling=True,\n",
    "                                             lemma=True\n",
    "                                             )\n",
    "print(processed_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T13:56:30.537034Z",
     "start_time": "2023-11-12T13:56:30.472314800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from preprocessor.Preprocessor import Preprocessor\n",
    "preprocessor = Preprocessor()\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"../dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"../dataset/test.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:55:50.936913700Z",
     "start_time": "2023-11-12T15:55:48.695088Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "# df_train20 = df_train[0:50]\n",
    "df_train20 = df_train[0:110]\n",
    "\n",
    "preprocessing_options = {\n",
    "    'lowercase': True,\n",
    "    'contractions': True,\n",
    "    'urls': True,\n",
    "    'punctuation': True,\n",
    "    'html_tags': True,\n",
    "    'emoji': True,\n",
    "    'spelling': True,\n",
    "    'abbreviations': True,\n",
    "    'lemma': True\n",
    "}\n",
    "\n",
    "\n",
    "df_train20_text = df_train20['text'].apply(lambda x: preprocessor.process_text(x, **preprocessing_options))\n",
    "# df_train20_location = df_train20['location'].apply(lambda x: preprocessor.process_text(x, **preprocessing_options))\n",
    "\n",
    "# df_train.head()\n",
    "\n",
    "df_result = pd.DataFrame({'text_df1': df_train20['text'], 'text_df2': df_train20_text})\n",
    "df_result.to_csv('df_concatenated.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:56:26.834646700Z",
     "start_time": "2023-11-12T15:56:15.641524300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# def separate_punctuations_from_words(self, text):\n",
    "#     continues_dots = ['........', '.......', '......','......', '.....', '....', '...', '..']\n",
    "#     exception_punctuation_list = [' ', '.', '?', '!',]\n",
    "#\n",
    "#     for char in text:\n",
    "#         if char not in exception_punctuation_list and not char.isalnum():\n",
    "#             text = text.replace(char, '')\n",
    "#     for c in continues_dots:\n",
    "#         text = text.replace(c, ' ... ')\n",
    "#     return text\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:48:16.102817100Z",
     "start_time": "2023-11-12T15:48:16.099306300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a goooooooaaaaaal ... ! ?\n"
     ]
    }
   ],
   "source": [
    "text_temp = 'What a            goooooooaaaaaal............!!!!!!  ????'\n",
    "import re\n",
    "def separate_punctuations_from_words(text):\n",
    "    exception_punctuation_list = [' ', '.', '?', '!', ]\n",
    "    for char in text:\n",
    "        if char not in exception_punctuation_list and not char.isalnum():\n",
    "            text = text.replace(char, '')\n",
    "\n",
    "    # replace continuous characters like . ! ?\n",
    "    text = re.sub(r'\\.{2,}', ' ... ', text)\n",
    "    for c in exception_punctuation_list:\n",
    "        if c != '.':\n",
    "            text = re.sub(rf'{re.escape(c)}{{2,}}', c, text)\n",
    "    return text\n",
    "\n",
    "print(separate_punctuations_from_words(text_temp))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:48:16.112771800Z",
     "start_time": "2023-11-12T15:48:16.106818100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'there be no victory at bargain basement price . ' right david eisenhower , there be no victory at bargain basement price . right david eisenhower\n"
     ]
    }
   ],
   "source": [
    "# text_temp = 'First night with retainers in. It\\'s quite weird. Better get used to it; I have to wear them every single night for the next year at least.'\n",
    "# text_temp = \"Hello, World! Visit https://www.example.com for more info.\"\n",
    "# text_temp = \"Horrible Accident Man Died In Wings Of ¬â√õ√èAirplane¬â√õ¬ù 29-07-2015. WTF You Can¬â√õ¬™t Believe Your EYES ¬â√õ√í... http://t.co/6fFyLAjWpS\"\n",
    "text_temp ='\\'There is no victory at bargain basement prices.\\' Dwight David Eisenhower,there be no victory at bargain basement price . right david eisenhower'\n",
    "text_processor = Preprocessor()\n",
    "processed_text = text_processor.process_text(text_temp,\n",
    "                                             spelling=True,\n",
    "                                             lemma=True\n",
    "                                             )\n",
    "print(processed_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T15:56:30.666492Z",
     "start_time": "2023-11-12T15:56:30.174509800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
